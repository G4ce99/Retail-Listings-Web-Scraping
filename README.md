# Retail-Listings-Web-Scraping
During the summer after my senior year in high school, I helped my dad with his work as a consultant for the states of Indiana and Georgia in the United States. At the time, he was manually scraping data off of a website called LoopNet, so I offered to write a program for him to simplify his work. This repository contains the two python files that I created to scrape all of the retail listings from the site for Georgia and Indiana. I used a VPN, an exponential backoff timer, and a header manipulation to trick and bypass the websiteâ€™s security system. Furthermore, I decided to scrape the data county by county, because the site was purposefully limiting the number of listings you could access with a single search. These programs output a csv file containing the county name, city name, address, and price for every retail listing in their respective state. The George dataset was so large that I decided to split it into thirds, create three csv files one at a time, and merge them into a single csv manually at the end. 
